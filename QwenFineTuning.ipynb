{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatinAminSabouri/deepseekv3_finetuning/blob/main/QwenFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTElatapdYKQ",
        "outputId": "d81666a8-9a73-491a-919a-c4e6c7cc1b92",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-yf42f9r9/unsloth_bf247369b9b24cb49d4d01da5c297161\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-yf42f9r9/unsloth_bf247369b9b24cb49d4d01da5c297161\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit aa5832de9282987ae6221dfac1877d23d64cad9a\n"
          ]
        }
      ],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "from google.colab import files\n",
        "import json\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "4vCnqqpSdcQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# آپلود فایل JSON\n",
        "uploaded = files.upload()\n",
        "\n",
        "# پردازش JSON\n",
        "def preprocess_json(json_data):\n",
        "    processed_data = []\n",
        "\n",
        "    # بررسی وجود sections\n",
        "    if \"sections\" not in json_data or not isinstance(json_data[\"sections\"], list):\n",
        "        print(\"Error: 'sections' key missing or not a list.\")\n",
        "        return processed_data\n",
        "\n",
        "    # گرفتن زمینه (context)\n",
        "    main_context = \"\"\n",
        "    if len(json_data[\"sections\"]) > 0:\n",
        "        main_context = json_data[\"sections\"][0].get(\"content\", \"\")\n",
        "\n",
        "    # گرفتن FAQها\n",
        "    faq_content = \"\"\n",
        "    if len(json_data[\"sections\"]) > 1:\n",
        "        faq_content = json_data[\"sections\"][1].get(\"content\", \"\")\n",
        "    else:\n",
        "        print(\"Warning: Only one section found. Looking for FAQ content in sections[0].\")\n",
        "        faq_content = json_data[\"sections\"][0].get(\"content\", \"\")\n",
        "\n",
        "    # پردازش faq_content\n",
        "    if isinstance(faq_content, str):\n",
        "        faq_lines = [line.strip().lstrip(\"- \") for line in faq_content.replace(\"- \", \"\\n\").split(\"\\n\") if line.strip()]\n",
        "        current_question = None\n",
        "        for line in faq_lines:\n",
        "            if line.endswith(\"؟\"):\n",
        "                current_question = line\n",
        "            elif current_question and line:\n",
        "                processed_data.append({\n",
        "                    \"question\": current_question,\n",
        "                    \"answer\": line,\n",
        "                    \"context\": main_context\n",
        "                })\n",
        "                current_question = None\n",
        "    elif isinstance(faq_content, list):\n",
        "        for item in faq_content:\n",
        "            if isinstance(item, dict) and item.get(\"section_type\") == \"faq\" and \"heading\" in item and \"content\" in item:\n",
        "                question = item[\"heading\"].replace(\"سوال\", \"\").strip().lstrip(\"0123456789: \")\n",
        "                answer = item[\"content\"].strip().lstrip(\"- \")\n",
        "                if question and answer:\n",
        "                    processed_data.append({\n",
        "                        \"question\": question,\n",
        "                        \"answer\": answer,\n",
        "                        \"context\": main_context\n",
        "                    })\n",
        "\n",
        "    # نمونه‌های منفی\n",
        "    negative_examples = [\n",
        "        {\"question\": \"پایتخت ایران کجاست؟\", \"answer\": \"من فقط می‌توانم درباره اطلاعات سامانه رهاورد پاسخ دهم.\", \"context\": main_context},\n",
        "        {\"question\": \"تو کی هستی؟\", \"answer\": \"من فقط می‌توانم درباره اطلاعات سامانه رهاورد پاسخ دهم.\", \"context\": main_context}\n",
        "    ]\n",
        "    processed_data.extend(negative_examples)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# پردازش همه فایل‌های JSON\n",
        "all_processed_data = []\n",
        "for dataset_file in uploaded.keys():\n",
        "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    all_processed_data.extend(preprocess_json(data))\n",
        "\n",
        "# تبدیل به دیتاست\n",
        "dataset = Dataset.from_list(all_processed_data)\n",
        "print(f\"Number of samples: {len(dataset)}\")\n",
        "if len(dataset) > 0:\n",
        "    print(\"First sample:\", dataset[0])\n",
        "else:\n",
        "    print(\"Error: No samples in dataset. Check JSON structure.\")"
      ],
      "metadata": {
        "id": "ESDB9fzhV1TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# پرامپت فاین‌تیونینگ (مشابه پرامپت تست)\n",
        "fine_tune_prompt = \"\"\"فقط چانک‌های ارائه‌شده را بررسی کن. اگر سؤالم دقیقاً با یکی از چانک‌ها مرتبط باشد (یعنی سؤالم یا کلمات کلیدی آن در چانک وجود داشته باشد)، آن چانک را بدون هیچ تغییر یا افزودن محتوای اضافی به‌عنوان پاسخ برگردان. اگر هیچ چانک مرتبطی پیدا نشد یا سؤالم بی‌ربط بود، فقط بگو: «من فقط می‌توانم درباره اطلاعات سامانه رهاورد پاسخ دهم.» تحت هیچ شرایطی محتوای جدید تولید نکن یا از دانش عمومی خودت استفاده نکن.\n",
        "\n",
        "### چانک‌ها:\n",
        "{context}\n",
        "\n",
        "### سؤالم:\n",
        "{question}\n",
        "\n",
        "### پاسخ:\n",
        "{answer}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for question, answer, context in zip(examples[\"question\"], examples[\"answer\"], examples[\"context\"]):\n",
        "        text = fine_tune_prompt.format(context=context, question=question, answer=answer) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# اعمال پرامپت به دیتاست\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "dataset = dataset.remove_columns([\"question\", \"answer\", \"context\"])\n",
        "print(\"Prompts formatted. Sample:\", dataset[0][\"text\"][:200] + \"...\")"
      ],
      "metadata": {
        "id": "Kk6FWTLmV62B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تنظیم LoRA برای فاین‌تیونینگ\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# تنظیمات آموزش\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,  # برای دیتاست کوچک\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"farsi_qwen_rag_model\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Training configured.\")"
      ],
      "metadata": {
        "id": "0KMAm_dBV8uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "print(\"Training completed. Stats:\", trainer_stats)"
      ],
      "metadata": {
        "id": "QzIT5MMKV-Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"farsi_qwen_rag_lora\")\n",
        "tokenizer.save_pretrained(\"farsi_qwen_rag_lora\")\n",
        "print(\"Model saved to 'farsi_qwen_rag_lora'\")"
      ],
      "metadata": {
        "id": "FQqUKVEvV_Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"farsi_qwen_rag_model\", 'zip', \"farsi_qwen_rag_lora\")\n",
        "files.download(\"farsi_qwen_rag_model.zip\")\n",
        "print(\"Model downloaded. For usage: load LoRA with PEFT and Unsloth.\")"
      ],
      "metadata": {
        "id": "jMeKn-HwWAfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# همان پرامپت سخت‌گیرانه\n",
        "concat_prompt = \"\"\"فقط چانک‌های ارائه‌شده را بررسی کن. اگر سؤالم دقیقاً با یکی از چانک‌ها مرتبط باشد (یعنی سؤالم یا کلمات کلیدی آن در چانک وجود داشته باشد)، آن چانک را بدون هیچ تغییر یا افزودن محتوای اضافی به‌عنوان پاسخ برگردان. اگر هیچ چانک مرتبطی پیدا نشد یا سؤالم بی‌ربط بود، فقط بگو: «من فقط می‌توانم درباره اطلاعات سامانه رهاورد پاسخ دهم.» تحت هیچ شرایطی محتوای جدید تولید نکن یا از دانش عمومی خودت استفاده نکن.\n",
        "\n",
        "### چانک‌ها:\n",
        "{}\n",
        "\n",
        "### سؤالم:\n",
        "{}\n",
        "\n",
        "### پاسخ:\n",
        "{}\"\"\"\n",
        "\n",
        "# چانک‌های نمونه\n",
        "test_chunks = \"\"\"\n",
        "- نوع درخواست باید براساس تقاضای درخواست‌کننده انتخاب شود. هر نوع درخواست روند و قوانین خاص خود را دارد.\n",
        "- مالک باید مجوز نقل و انتقال را دریافت کند. این مجوز معمولاً طی چند روز تا یک هفته صادر می‌شود و مدت اعتبار آن بین یک تا سه ماه است.\n",
        "- بله، درخواست باید روی یک پرونده موجود ثبت شود. بنابراین ابتدا لازم است پرونده تشکیل شود.\n",
        "\"\"\"\n",
        "\n",
        "# سؤالم\n",
        "#test_question = \"پایتخت ایران کجاست؟\"  # باید پیغام خطا بده\n",
        "#test_question = \"نوع درخواست چگونه انتخاب می‌شود؟\"  # باید چانک مرتبط رو برگردونه\n",
        "test_question = \"آیا قبل از ثبت درخواست، تشکیل پرونده الزامی است؟\"\n",
        "\n",
        "# فیلتر کردن چانک‌ها\n",
        "def filter_chunk(question, chunks):\n",
        "    if not chunks.strip():\n",
        "        return None\n",
        "    chunk_lines = [line.strip().lstrip(\"- \") for line in chunks.split(\"\\n\") if line.strip()]\n",
        "    for chunk in chunk_lines:\n",
        "        if any(word in chunk for word in question.split()):\n",
        "            return chunk\n",
        "    return None\n",
        "\n",
        "selected_chunk = filter_chunk(test_question, test_chunks)\n",
        "\n",
        "# ایجاد پرامپت\n",
        "if selected_chunk:\n",
        "    test_prompt = concat_prompt.format(test_chunks, test_question, selected_chunk)\n",
        "else:\n",
        "    test_prompt = concat_prompt.format(test_chunks, test_question, \"من فقط می‌توانم درباره اطلاعات سامانه رهاورد پاسخ دهم.\")\n",
        "\n",
        "# تولید پاسخ\n",
        "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True, temperature=0.00001, top_p=0.9)\n",
        "generated = tokenizer.batch_decode(outputs)[0]\n",
        "print(\"پاسخ تولیدشده:\", generated.split(\"### پاسخ:\")[-1].strip())"
      ],
      "metadata": {
        "id": "9gQR9aSPWDK6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}